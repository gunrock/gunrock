# PyGunrock Python Interface Workflow
name: PyGunrock

# Controls when the workflow will run
on:
  # Triggers the workflow on push or pull request events but only for the main branch
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

env:
  BUILD_TYPE: Release
  PYTHON_VERSION: "3.10"
  
jobs:
  build:
    strategy:
      matrix:
        os: [ubuntu-22.04]
        backend: [nvidia, amd]
        include:
          - backend: nvidia
            architectures: "90"
            cmake_arch_flag: "-DCMAKE_CUDA_ARCHITECTURES=90"
            torch_index_url: "https://download.pytorch.org/whl/cu124"
          - backend: amd
            architectures: "gfx950"
            cmake_arch_flag: "-DCMAKE_HIP_ARCHITECTURES=gfx950"
            torch_index_url: "https://download.pytorch.org/whl/rocm6.2"
        
    # https://github.blog/changelog/2021-02-08-github-actions-skip-pull-request-and-push-workflows-with-skip-ci/
    if: "!contains(github.event.commits[0].message, '[skip pygunrock]')"
    runs-on: ${{matrix.os}}
    name: Build PyGunrock (${{matrix.backend}})

    steps:
      # Fetch CUDA toolkit for NVIDIA backend
      - name: Install CUDA toolkit via apt
        if: matrix.backend == 'nvidia'
        run: |
          # Clean up any existing CUDA installations
          sudo apt-get remove --purge -y "cuda*" "nvidia-cuda-toolkit" "nvidia-*" 2>/dev/null || true
          sudo rm -rf /usr/local/cuda* /etc/profile.d/cuda.sh
          
          # Add NVIDIA CUDA repository
          wget -q https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin
          sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600
          wget -q https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub
          sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/cuda-archive-keyring.gpg 3bf863cc.pub
          echo "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/ /" | sudo tee /etc/apt/sources.list.d/cuda.list
          sudo apt-get update
          
          # Install CUDA toolkit (version 12.4)
          sudo apt-get install -y cuda-toolkit-12-4
          
          # Find where CUDA was actually installed
          CUDA_PATH=$(ls -d /usr/local/cuda-12.* 2>/dev/null | head -1)
          if [ -z "$CUDA_PATH" ]; then
            CUDA_PATH="/usr/local/cuda-12.4"
          fi
          
          # Create symlink if it doesn't exist
          if [ ! -L /usr/local/cuda ]; then
            sudo ln -sf "$CUDA_PATH" /usr/local/cuda
          fi
          
          # Set up PATH for subsequent steps (must use $GITHUB_PATH)
          echo "$CUDA_PATH/bin" >> $GITHUB_PATH
          echo "CUDA_HOME=$CUDA_PATH" >> $GITHUB_ENV
          echo "LD_LIBRARY_PATH=$CUDA_PATH/lib64:$LD_LIBRARY_PATH" >> $GITHUB_ENV
          
          # Verify installation in this step
          "$CUDA_PATH/bin/nvcc" --version

      - name: Check nvcc version
        if: matrix.backend == 'nvidia'
        run: nvcc -V

      # Install ROCm for AMD backend
      - name: Install ROCm
        if: matrix.backend == 'amd'
        uses: loostrum/rocm-installer@v0.2
        with:
          version: latest
          packages: rocm-hip-runtime-dev rocprim-dev rocthrust-dev hipcub-dev hiprand-dev hipsparse-dev rocrand roctracer

      - name: Check hipcc version
        if: matrix.backend == 'amd'
        run: hipcc --version
        
      - uses: actions/checkout@v6

      # Set up Python
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install nanobind scikit-build-core pytest pytest-cov

      # Install PyTorch with appropriate backend
      - name: Install PyTorch (${{matrix.backend}})
        run: |
          pip install torch --index-url ${{matrix.torch_index_url}}

      - name: Verify PyTorch installation
        run: |
          python -c "import torch; print(f'PyTorch version: {torch.__version__}')"
          python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"

      # Build PyGunrock with pip install
      - name: Build PyGunrock (AMD)
        if: matrix.backend == 'amd'
        env:
          LD_LIBRARY_PATH: /opt/rocm/lib
        working-directory: ${{github.workspace}}/python
        run: |
          CMAKE_ARGS="${{matrix.cmake_arch_flag}} -DCMAKE_PREFIX_PATH=/opt/rocm" pip install -e . -v

      - name: Build PyGunrock (NVIDIA)
        if: matrix.backend == 'nvidia'
        working-directory: ${{github.workspace}}/python
        run: |
          CMAKE_ARGS="${{matrix.cmake_arch_flag}}" pip install -e . -v

      # Verify installation
      - name: Verify PyGunrock installation
        run: |
          python -c "import sys; sys.path.insert(0, 'python/src'); import gunrock; print(f'PyGunrock version: {gunrock.__version__}')"

      # Run tests
      - name: Run PyGunrock tests
        working-directory: ${{github.workspace}}/python
        run: |
          pytest tests/ -v --cov=gunrock --cov-report=xml --cov-report=term

      # Upload coverage reports
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          files: ./python/coverage.xml
          flags: pygunrock-${{matrix.backend}}
          name: pygunrock-${{matrix.backend}}
          fail_ci_if_error: false
